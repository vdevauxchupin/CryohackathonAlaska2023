{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e54115a3-ecaa-4131-bcb3-77f84831f91f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div>\n",
    "    <h2><center>Global Glacier Velocity Data Downloading</center></h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d2616c",
   "metadata": {},
   "source": [
    "# Setting up a local environment\n",
    "\n",
    "<br>\n",
    "\n",
    "At the terminal:\n",
    ">conda create --name Glaciervel -c conda-forge h5netcdf fiona shapely jupyter netcdf4 psutil h5py zarr matplotlib gdal xarray  boto3 pyproj ipympl ipyleaflet s3fs geopandas rasterio seaborn markdown\n",
    "\n",
    "<br>\n",
    "\n",
    "Activate newly created environment:\n",
    "> conda activate Glaciervel\n",
    "\n",
    "<br>\n",
    "\n",
    "Start jupyter in browser\n",
    "> jupyter notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68c09a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the path for essential python scripts\n",
    "\n",
    "import os\n",
    "import sys\n",
    "path =  os.getcwd()\n",
    "# Avoid Windows users to have issues with how paths are written\n",
    "path = path.replace('\\\\','/')\n",
    "\n",
    "# Import python scripts from scripts folder\n",
    "sys.path.append(path + '/scripts')\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "330a2f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60.29974250173091, -139.51584366860752]\n",
      "click\n",
      "[60.29974250173091, -139.51584366860752]\n",
      "0 (0.12156862745098039, 0.4666666666666667, 0.7058823529411765, 1.0)\n",
      "point added [60.29974250173091, -139.51584366860752]\n",
      "[59.98250201795759, -139.285095942879]\n",
      "click\n",
      "[59.98250201795759, -139.285095942879]\n",
      "1 (1.0, 0.4980392156862745, 0.054901960784313725, 1.0)\n",
      "point added [59.98250201795759, -139.285095942879]\n"
     ]
    }
   ],
   "source": [
    "# Import all the necessary packages\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import markdown\n",
    "from velocity_widget import ITSLIVE\n",
    "matplotlib.rcParams['figure.figsize'] = [9, 5]\n",
    "matplotlib.rcParams[\"figure.autolayout\"] = True\n",
    "velocity_widget = ITSLIVE()\n",
    "plt.close()\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import time\n",
    "from pyproj import Transformer\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "import geopandas as gpd\n",
    "from ipywidgets import widgets, HTML, Output\n",
    "import numpy as np\n",
    "import math\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ad345d-087a-43e8-bf24-1e0be4ba6565",
   "metadata": {},
   "source": [
    "## Instructions: \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**First Step:**\n",
    "\n",
    "<br>\n",
    "\n",
    "- Select type_dataset: \"Yearly\" or \"Subyearly\" (\"Subryearly\" does not necessarily cover every day, just has a higher time resolution). Yearly datasets can be much bigger spatially than the Subyearly ones (because they have only 1 time dimension, while Subyearly dataset might have thousands).\n",
    "- Select a starting date and ending date (format 'yyyy-mm-dd').\n",
    "- If you chose \"Subyearly\", consider changing the \"threshold\" value. \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### How to use the interactive map\n",
    " \n",
    "<br>\n",
    "\n",
    "Run the next two cells, and wait for the map to appear.\n",
    "\n",
    "<br>\n",
    "\n",
    "If you chose \"Yearly\" dataset:\n",
    ">- Select an Area Of Interest (AOI) by placing two markers on the map with a right click. Once you have >two markers, a blue rectangle representing your AOI will be plotted. If you made a mistake, plot a >rectangle then click on \"Clear points\".\n",
    "\n",
    "<br>\n",
    "\n",
    "If you chose \"Subyearly\" dataset:\n",
    ">- Same as for the \"Yearly\" dataset: draw a rectangle with two markers (right click).\n",
    ">- Once the rectangle has been drawn, put at least 1 point in every red rectangle intersecting with the blue AOI (double left-click).\n",
    ">- Click on the \"Get points\".\n",
    ">- Once prompted, follow the next step.\n",
    "\n",
    "<br>\n",
    "\n",
    "**To download the data**\n",
    "Click on \"Export data\". Once the message saying it has finished downloading has appeared, you can plot it to see what you downloaded. *Be aware, downloading will take some time*.\n",
    "\n",
    "<br>\n",
    "\n",
    "**To plot the data**\n",
    "Click on the blue 'plot' button when prompted (after exporting the data).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a32b1d9",
   "metadata": {},
   "source": [
    "**MAKE SURE THAT:**\n",
    "\n",
    "<br>\n",
    "\n",
    "- You keep the \"Subyearly\" datacubes reasonnably small: downloading them takes a lot of memory. I encourage to slice them by date selection. You can also change the threshold set to 40% (meaning it will keep only the time slices with 40% or more data, discard the rest)\n",
    "- All the points you want are in the Area Of Interest (AOI) for which you chose the boundaries' coordinates.\n",
    "- Don't worry if it takes time to download the data, these are big dataset.\n",
    "\n",
    "<br>\n",
    "\n",
    "##### As of the 26/10/22, \"Yearly\" dataset are available from 1985 until 2018 included. If your date range is bigger than that, the code will still run and give an error because the dataset are not available. \"Subyearly\" dataset are available between 2013 to now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33928f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select the type of dataset you want to download: 'Yearly', 'Subyearly' or 'Composite'\n",
    "type_dataset = \"Subyearly\"\n",
    "\n",
    "# Select your date range (careful the datacubes get heavy quickly)\n",
    "sdate = '2016-02-18'\n",
    "edate = '2019-12-31'\n",
    "\n",
    "# Select Threshold quality\n",
    "threshold = 40\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a8829c",
   "metadata": {},
   "source": [
    "#### **Select the variables you want to keep:**\n",
    "\n",
    "<br>\n",
    "\n",
    "Depending on which dataset you want to download ('Subyearly' or 'Yearly'), the available variables might change. Because of the size of the files to download, it is advised to \"trim down\" the datacube by downloading *only the variables in which you are interested*. For that, select your variables of interest from the list below. Copy-paste them in the list, in the next cell. If you want to keep all the variables, just keep that list empty ! \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**'Subyearly' variables:**\n",
    "\n",
    "\n",
    ">['acquisition_date_img1',\n",
    " 'acquisition_date_img2',\n",
    " 'autoRIFT_software_version',\n",
    " 'chip_size_height',\n",
    " 'chip_size_width',\n",
    " 'date_center',\n",
    " 'date_dt',\n",
    " 'granule_url',\n",
    " 'interp_mask',\n",
    " 'mapping',\n",
    " 'mission_img1',\n",
    " 'mission_img2',\n",
    " 'roi_valid_percentage',\n",
    " 'satellite_img1',\n",
    " 'satellite_img2',\n",
    " 'sensor_img1',\n",
    " 'sensor_img2',\n",
    " 'stable_count_mask',\n",
    " 'stable_count_slow',\n",
    " 'stable_shift_flag',\n",
    " 'v',\n",
    " 'v_error',\n",
    " 'va',\n",
    " 'va_error',\n",
    " 'va_error_mask',\n",
    " 'va_error_modeled',\n",
    " 'va_error_slow',\n",
    " 'va_stable_shift',\n",
    " 'va_stable_shift_mask',\n",
    " 'va_stable_shift_slow',\n",
    " 'vr',\n",
    " 'vr_error',\n",
    " 'vr_error_mask',\n",
    " 'vr_error_modeled',\n",
    " 'vr_error_slow',\n",
    " 'vr_stable_shift',\n",
    " 'vr_stable_shift_mask',\n",
    " 'vr_stable_shift_slow',\n",
    " 'vx',\n",
    " 'vx_error',\n",
    " 'vx_error_mask',\n",
    " 'vx_error_modeled',\n",
    " 'vx_error_slow',\n",
    " 'vx_stable_shift',\n",
    " 'vx_stable_shift_mask',\n",
    " 'vx_stable_shift_slow',\n",
    " 'vy',\n",
    " 'vy_error',\n",
    " 'vy_error_mask',\n",
    " 'vy_error_modeled',\n",
    " 'vy_error_slow',\n",
    " 'vy_stable_shift',\n",
    " 'vy_stable_shift_mask',\n",
    " 'vy_stable_shift_slow']\n",
    " \n",
    " <br>\n",
    " \n",
    " **'Yearly' variables:**\n",
    " \n",
    ">['vx',\n",
    " 'vy',\n",
    " 'v',\n",
    " 'vx_err',\n",
    " 'vy_err',\n",
    " 'v_err',\n",
    " 'date',\n",
    " 'dt',\n",
    " 'count',\n",
    " 'chip_size_max',\n",
    " 'ocean',\n",
    " 'rock',\n",
    " 'ice',\n",
    " 'Polar_Stereographic']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19b90e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, here I want only the velocity components\n",
    "variables_to_keep = ['vx','vy']\n",
    "\n",
    "# If you want to download all the variables, run the following line:\n",
    "#variables_to_keep = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e3794f",
   "metadata": {},
   "source": [
    "#### Run the following Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc8e3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6747774f188d4bd98769b326168c2543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(VBox(children=(Map(center=[57.2, -49.43], controls=(ZoomControl(options=['position', 'zoom_iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fetching timeseries for point x=   -139.52 y=     60.30\n",
      "original xy [-139.51584366860752, 60.29974250173091] 4326 maps to datacube (-3278263.2403743276, 258917.0663173867) EPSG:3413\n",
      "[-139.51584366860752, 60.29974250173091]\n",
      "elapsed time:       5.36 - 10328.6 points per second\n",
      "fetching timeseries for point x=   -139.29 y=     59.98\n",
      "original xy [-139.285095942879, 59.98250201795759] 4326 maps to datacube (-3315891.4906964255, 248455.72822120818) EPSG:3413\n",
      "[-139.285095942879, 59.98250201795759]\n",
      "elapsed time:       4.85 - 6987.3 points per second\n",
      "Done. You can export the data.\n",
      "Downloading...\n",
      "downloaded ITS_LIVE_vel_EPSG3413_G0120_X-3250000_Y250000 spatial slice    869.9 seconds\n",
      "downloaded ITS_LIVE_vel_EPSG3413_G0120_X-3350000_Y250000 spatial slice    195.6 seconds\n",
      "Done ! You can hit \"plot\" now\n",
      "Downloading...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 26.8 GiB for an array with shape (13346, 794, 680) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 233\u001b[0m, in \u001b[0;36mdownloader\u001b[1;34m(whatever)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[39m# Load datacube according to prerequisites (time, space and variables)\u001b[39;00m\n\u001b[0;32m    227\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    228\u001b[0m xrds \u001b[39m=\u001b[39m xr\u001b[39m.\u001b[39;49mopen_dataset(url,\n\u001b[0;32m    229\u001b[0m                         engine\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mzarr\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m    230\u001b[0m                         drop_variables\u001b[39m=\u001b[39;49mvariables_drop\n\u001b[0;32m    231\u001b[0m                         )\u001b[39m.\u001b[39;49msel(mid_date\u001b[39m=\u001b[39;49mt_mask,\n\u001b[0;32m    232\u001b[0m                             x\u001b[39m=\u001b[39;49m\u001b[39mslice\u001b[39;49m(velocity_widget\u001b[39m.\u001b[39;49mxmin_proj, velocity_widget\u001b[39m.\u001b[39;49mxmax_proj),\n\u001b[1;32m--> 233\u001b[0m                             y\u001b[39m=\u001b[39;49m\u001b[39mslice\u001b[39;49m(velocity_widget\u001b[39m.\u001b[39;49mymax_proj, velocity_widget\u001b[39m.\u001b[39;49mymin_proj))\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m    235\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdownloaded \u001b[39m\u001b[39m{\u001b[39;00mcubes[n]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m spatial slice \u001b[39m\u001b[39m{\u001b[39;00mtime\u001b[39m.\u001b[39mtime()\u001b[39m-\u001b[39mstart\u001b[39m:\u001b[39;00m\u001b[39m8.1f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m seconds\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    236\u001b[0m xrds\u001b[39m.\u001b[39mto_netcdf(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpathsave\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00mcubes[n]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00msdate\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00medate\u001b[39m}\u001b[39;00m\u001b[39m.nc\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32ma:\\System\\Users\\whisk\\anaconda3\\envs\\Glaciervel\\lib\\site-packages\\xarray\\core\\dataset.py:743\u001b[0m, in \u001b[0;36mDataset.load\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    741\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvariables\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    742\u001b[0m     \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m lazy_data:\n\u001b[1;32m--> 743\u001b[0m         v\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m    745\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32ma:\\System\\Users\\whisk\\anaconda3\\envs\\Glaciervel\\lib\\site-packages\\xarray\\core\\variable.py:493\u001b[0m, in \u001b[0;36mVariable.load\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data \u001b[39m=\u001b[39m as_compatible_data(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data\u001b[39m.\u001b[39mcompute(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs))\n\u001b[0;32m    492\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_duck_array(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data):\n\u001b[1;32m--> 493\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data)\n\u001b[0;32m    494\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32ma:\\System\\Users\\whisk\\anaconda3\\envs\\Glaciervel\\lib\\site-packages\\xarray\\core\\indexing.py:653\u001b[0m, in \u001b[0;36mMemoryCachedArray.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    652\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 653\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ensure_cached()\n\u001b[0;32m    654\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39masarray(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39marray, dtype\u001b[39m=\u001b[39mdtype)\n",
      "File \u001b[1;32ma:\\System\\Users\\whisk\\anaconda3\\envs\\Glaciervel\\lib\\site-packages\\xarray\\core\\indexing.py:650\u001b[0m, in \u001b[0;36mMemoryCachedArray._ensure_cached\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    648\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_ensure_cached\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    649\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39marray, NumpyIndexingAdapter):\n\u001b[1;32m--> 650\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39marray \u001b[39m=\u001b[39m NumpyIndexingAdapter(np\u001b[39m.\u001b[39;49masarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49marray))\n",
      "File \u001b[1;32ma:\\System\\Users\\whisk\\anaconda3\\envs\\Glaciervel\\lib\\site-packages\\xarray\\core\\indexing.py:623\u001b[0m, in \u001b[0;36mCopyOnWriteArray.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    622\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 623\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49masarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49marray, dtype\u001b[39m=\u001b[39;49mdtype)\n",
      "File \u001b[1;32ma:\\System\\Users\\whisk\\anaconda3\\envs\\Glaciervel\\lib\\site-packages\\xarray\\core\\indexing.py:524\u001b[0m, in \u001b[0;36mLazilyIndexedArray.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    523\u001b[0m     array \u001b[39m=\u001b[39m as_indexable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39marray)\n\u001b[1;32m--> 524\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49masarray(array[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey], dtype\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[1;32ma:\\System\\Users\\whisk\\anaconda3\\envs\\Glaciervel\\lib\\site-packages\\xarray\\coding\\variables.py:72\u001b[0m, in \u001b[0;36m_ElementwiseFunctionArray.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m---> 72\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49marray)\n",
      "File \u001b[1;32ma:\\System\\Users\\whisk\\anaconda3\\envs\\Glaciervel\\lib\\site-packages\\xarray\\coding\\variables.py:139\u001b[0m, in \u001b[0;36m_apply_mask\u001b[1;34m(data, encoded_fill_values, decoded_fill_value, dtype)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply_mask\u001b[39m(\n\u001b[0;32m    136\u001b[0m     data: np\u001b[39m.\u001b[39mndarray, encoded_fill_values: \u001b[39mlist\u001b[39m, decoded_fill_value: Any, dtype: Any\n\u001b[0;32m    137\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[0;32m    138\u001b[0m     \u001b[39m\"\"\"Mask all matching values in a NumPy arrays.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m     data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(data, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    140\u001b[0m     condition \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     \u001b[39mfor\u001b[39;00m fv \u001b[39min\u001b[39;00m encoded_fill_values:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 26.8 GiB for an array with shape (13346, 794, 680) and data type float32"
     ]
    }
   ],
   "source": [
    "   \n",
    "dates_range = widgets.SelectionRangeSlider(\n",
    "    options=[i for i in range(546)],\n",
    "    index=(1, 120),\n",
    "    continuous_update=False,\n",
    "    description='Interval (days): ',\n",
    "    orientation='horizontal',\n",
    "    layout={'width': '90%',\n",
    "            'display': 'flex'},\n",
    "    style={'description_width': 'initial'})\n",
    "\n",
    "variables =  widgets.Dropdown(\n",
    "    options=['v', 'v_error', 'vx', 'vy'],\n",
    "    description='Variable: ',\n",
    "    disabled=False,\n",
    "    value='v',\n",
    "    layout={'width': '20%',\n",
    "            'display': 'flex'},\n",
    "    style={'description_width': 'initial'})\n",
    "\n",
    "plot_type =  widgets.Dropdown(\n",
    "    options=['location', 'satellite'],\n",
    "    description='Plot By: ',\n",
    "    disabled=False,\n",
    "    value='location',\n",
    "    layout={'width': '20%',\n",
    "            'display': 'flex'},\n",
    "    style={'description_width': 'initial'})\n",
    "\n",
    "plot_button =  widgets.Button(\n",
    "    description='Plot',\n",
    "    button_style='primary',\n",
    "    icon='line-chart',\n",
    "    style={'description_width': 'initial'})\n",
    "\n",
    "get_points =  widgets.Button(\n",
    "    description='Get points',\n",
    "    button_style='primary',\n",
    "    icon='line-chart',\n",
    "    style={'description_width': 'initial'})\n",
    "\n",
    "clear_button =  widgets.Button(\n",
    "    description='Clear Points',\n",
    "    # button_style='warning',\n",
    "    icon=\"trash\",\n",
    "    style={'description_width': 'initial'})\n",
    "\n",
    "latitude = widgets.BoundedFloatText(\n",
    "    value=0.0,\n",
    "    min=-90.0,\n",
    "    max=90.0,\n",
    "    step=0.1,\n",
    "    description='Lat: ',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'},\n",
    "    layout={'width': '20%',\n",
    "            'display': 'flex'},\n",
    ")\n",
    "\n",
    "longitude = widgets.BoundedFloatText(\n",
    "    value=0.0,\n",
    "    min=-180.0,\n",
    "    max=180.0,\n",
    "    step=0.1,\n",
    "    description='Lon: ',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'},\n",
    "    layout={'width': '20%',\n",
    "            'display': 'flex'},\n",
    ")\n",
    "\n",
    "add_button =  widgets.Button(\n",
    "    description='Add Point',\n",
    "    # button_style='info',\n",
    "    icon=\"map-marker\",\n",
    "    style={'description_width': 'initial'})\n",
    "\n",
    "include_running_mean =  widgets.Checkbox(\n",
    "            value=False,\n",
    "            description=\"Include Running Mean\",\n",
    "            style={'description_width': 'initial'},\n",
    "            disabled=False,\n",
    "            indent=False,\n",
    "            tooltip=\"Plot running mean through each time series\",\n",
    "            layout=widgets.Layout(width=\"25%\"),\n",
    "        )\n",
    "\n",
    "export_button = widgets.Button(\n",
    "    description='Export Data',\n",
    "    # button_style='info',\n",
    "    icon=\"file-export\",\n",
    "    style={'description_width': 'initial'})\n",
    "\n",
    "data_link = widgets.HTML(\n",
    "    value=\"<br>\"\n",
    ")\n",
    "\n",
    "# If this congiguration changes we need to rerun the cell.\n",
    "config = { \n",
    "    \"plot\": \"v\", # or other ITS_LIVE variables: vx, vy ...\n",
    "    \"min_separation_days\": 1,\n",
    "    \"max_separation_days\": 90,\n",
    "    \"color_by\": \"location\", # valid values: satellite, points\n",
    "    \"verbose\": True, # print operations\n",
    "    \"runnig_mean\": True,\n",
    "    \"coords\": {\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude\n",
    "    },\n",
    "    \"data_link\": data_link\n",
    "}\n",
    "\n",
    "# This function is triggered when hitting the 'Export Data' button. It downloads the datacubes\n",
    "# according to the user's choices (date range, AOI, data type, variables to keep)\n",
    "def downloader(whatever):\n",
    "    # Import the choice of variables from the user\n",
    "    global variables_to_keep\n",
    "    print('Downloading...')\n",
    "    global pathsave\n",
    "    ######### YEARLY DATASET DOWNLOAD ########\n",
    "    if type_dataset == 'Yearly':\n",
    "\n",
    "            # Create list of years for the date range chosen earlier\n",
    "            list_years = np.arange(int(sdate.split('-')[0]), int(edate.split('-')[0])+1)\n",
    "\n",
    "            # Create path to the files\n",
    "            pathsave = velocity_widget.path_yearly_datacubes\n",
    "            os.makedirs(pathsave, exist_ok = True)\n",
    "            \n",
    "            # Update the list of variables to keep. Unfortunately when printing the keys of the datacubes,\n",
    "            # the dimensions don't show up in the list. But if you drop them, the file won't download.\n",
    "            # This is why we add them to the list of variables to keep\n",
    "            variables_to_keep += ['mid_date', 'x', 'y']\n",
    "            \n",
    "            # List of variables to drop for the download (we drop everything but the variables written below)\n",
    "            variables_drop = [ele for ele in list(\n",
    "                    xr.open_dataset(f'{velocity_widget.url_region_yearly[0]}{int(list_years[0])}.nc#mode=bytes').variables\n",
    "                    ) if ele not in variables_to_keep\n",
    "                             ]\n",
    "                              \n",
    "\n",
    "            for Y in range(len(list_years)):\n",
    "\n",
    "                    # Generate URL for the nc file\n",
    "                    url = f'{velocity_widget.url_region_yearly[0]}{int(list_years[Y])}.nc#mode=bytes'\n",
    "\n",
    "                    # Load datacube according to prerequisites (time, space and variables)\n",
    "                    start = time.time()\n",
    "                    xrds = xr.open_dataset(url,\n",
    "                                           drop_variables=variables_drop\n",
    "                                            ).sel(x=slice(velocity_widget.xmin_proj, velocity_widget.xmax_proj),\n",
    "                                                y=slice(velocity_widget.ymax_proj, velocity_widget.ymin_proj)).load()\n",
    "\n",
    "                    print(f\"downloaded {list_years[Y]} spatial slice {time.time()-start:8.1f} seconds\")\n",
    "                    xrds.to_netcdf(f\"{pathsave}{list_years[Y]}.nc\")\n",
    "\n",
    "\n",
    "    ######## Composite DATASET DOWNLOAD ########\n",
    "    elif type_dataset == 'Composite':\n",
    "\n",
    "            # Create path to the file\n",
    "            pathsave = velocity_widget.path_composite_datacubes\n",
    "            os.makedirs(pathsave, exist_ok = True)\n",
    "            \n",
    "            # Update the list of variables to keep. Unfortunately when printing the keys of the datacubes,\n",
    "            # the dimensions don't show up in the list. But if you drop them, the file won't download.\n",
    "            # This is why we add them to the list of variables to keep\n",
    "            variables_to_keep += ['mid_date', 'x', 'y']\n",
    "            \n",
    "            # List of variables to drop for the download (we drop everything but the variables written below)\n",
    "            variables_drop = [ele for ele in list(\n",
    "                    xr.open_dataset(f'{velocity_widget.url_region_composite[0]}.nc#mode=bytes').variables\n",
    "                    ) if ele not in variables_to_keep\n",
    "                             ]\n",
    "                              \n",
    "            # Generate URL for the nc file\n",
    "            url = f'{velocity_widget.url_region_composite[0]}.nc#mode=bytes'\n",
    "\n",
    "            # Load datacube according to prerequisites (time, space and variables)\n",
    "            start = time.time()\n",
    "            xrds = xr.open_dataset(url,\n",
    "                                    drop_variables=variables_drop\n",
    "                                    ).load()\n",
    "\n",
    "            print(f\"downloaded composite {velocity_widget.name_region[0]} spatial slice {time.time()-start:8.1f} seconds\")\n",
    "            xrds.to_netcdf(f\"{pathsave}{velocity_widget.name_region[0]}.nc\")   \n",
    "\n",
    "\n",
    "\n",
    "    ######## Subyearly DATASET DOWNLOAD ########\n",
    "    elif type_dataset == 'Subyearly':\n",
    "\n",
    "            # Create path to the files\n",
    "            pathsave = velocity_widget.path_subyearly_datacubes\n",
    "            os.makedirs(pathsave, exist_ok = True)\n",
    "\n",
    "            # Get the cube address\n",
    "            cubes = velocity_widget.dct.addresses\n",
    "            cubes = [*set(cubes)]\n",
    "\n",
    "            # Update the list of variables to keep. Unfortunately when printing the keys of the datacubes,\n",
    "            # the dimensions don't show up in the list. But if you drop them, the file won't download.\n",
    "            # This is why we add them to the list of variables to keep\n",
    "            variables_to_keep += ['mid_date', 'x', 'y']\n",
    "            \n",
    "            # List of variables to drop for the download (we drop everything but the variables written below)\n",
    "            variables_drop = [ele for ele in list(\n",
    "                    xr.open_dataset(cubes[0], engine='zarr').variables\n",
    "                    ) if ele not in variables_to_keep\n",
    "            ]\n",
    "\n",
    "            \n",
    "            for n in range(len(cubes)):\n",
    "\n",
    "                    # Get the cube's URL\n",
    "                    url = cubes[n]\n",
    "\n",
    "                    # Load indices of slices above the quality threshold\n",
    "                    valid = xr.open_dataset(cubes[n], engine='zarr').roi_valid_percentage.values\n",
    "\n",
    "                    # Grab the time values\n",
    "                    t = xr.open_dataset(cubes[n], engine='zarr').mid_date.values\n",
    "\n",
    "                    # Create a time mask, based on the validity of layers and the custom date-range\n",
    "                    t_mask = np.logical_and(valid>threshold, np.logical_and(t>np.datetime64(sdate), t<np.datetime64(edate)))\n",
    "                    \n",
    "                    # Load datacube according to prerequisites (time, space and variables)\n",
    "                    start = time.time()\n",
    "                    xrds = xr.open_dataset(url,\n",
    "                                            engine='zarr',\n",
    "                                            drop_variables=variables_drop\n",
    "                                            ).sel(mid_date=t_mask,\n",
    "                                                x=slice(velocity_widget.xmin_proj, velocity_widget.xmax_proj),\n",
    "                                                y=slice(velocity_widget.ymax_proj, velocity_widget.ymin_proj)).load()\n",
    "\n",
    "                    print(f\"downloaded {cubes[n].split('/')[-1].split('.')[0]} spatial slice {time.time()-start:8.1f} seconds\")\n",
    "                    xrds.to_netcdf(f\"{pathsave}{cubes[n].split('/')[-1].split('.')[0]}_{sdate}_{edate}.nc\")\n",
    "    print('Done ! You can hit \"plot\" now')\n",
    "\n",
    "# This function plots the first variable defined by the user in \"variables_to_keep\" to ensure that \n",
    "# everything was downloaded. Sometimes if the internet connection is bad, the datacubes will be downloaded\n",
    "# incompletely without giving an error message. Plotting them helps determine if they are complete.\n",
    "# We plot the nanmean of files. If one files is not downloaded completely because of an internet connection,\n",
    "# the holes in the dataset will be per region and not per time. Hence, a complete file should \n",
    "# look like the mosaic that is plotted on the map. If there are holes, try again with a better internet connection.\n",
    "\n",
    "def plotter(whatever):\n",
    "    # Import the variable\n",
    "    global variables_to_keep\n",
    "    \n",
    "    # Print the list of files we just downloaded\n",
    "    list_files = glob.glob(f'{pathsave}*.nc')\n",
    "\n",
    "    fig, ax = plt.subplots(len(list_files), figsize=(10,10*len(list_files)))\n",
    "\n",
    "    # Depending on the number of files downloaded, we plot several figures to make sure every \n",
    "    # dataset was downloaded completely.\n",
    "    if len(list_files) == 1:\n",
    "        if type_dataset == \"Yearly\":\n",
    "            ax.pcolormesh(xr.open_dataset(list_files[0]).x.values,xr.open_dataset(list_files[0]).y.values,xr.open_dataset(list_files[0])[variables_to_keep[0]].values)\n",
    "        else:\n",
    "            ax.pcolormesh(xr.open_dataset(list_files[0]).x.values,xr.open_dataset(list_files[0]).y.values,np.nanmean(xr.open_dataset(list_files[0])[variables_to_keep[0]].values,axis = 0))\n",
    "    else:\n",
    "        if type_dataset == \"Yearly\":\n",
    "            for i in range(len(list_files)):\n",
    "                ax[i].pcolormesh(xr.open_dataset(list_files[i]).x.values,xr.open_dataset(list_files[i]).y.values,xr.open_dataset(list_files[i])[variables_to_keep[0]].values)\n",
    "        else:\n",
    "            for i in range(len(list_files)):\n",
    "                ax[i].pcolormesh(xr.open_dataset(list_files[i]).x.values,xr.open_dataset(list_files[i]).y.values,np.nanmean(xr.open_dataset(list_files[i])[variables_to_keep[0]].values,axis = 0))\n",
    "\n",
    "\n",
    "### --- These functions are for the widget --- ###\n",
    "def update_variable(change):\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            config[\"plot\"] = variables.value\n",
    "            velocity_widget.set_config(config)\n",
    "            velocity_widget.plot_time_series()\n",
    "            \n",
    "def update_range(change):\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            start, end = change['new']\n",
    "            config[\"min_separation_days\"] = start\n",
    "            config[\"max_separation_days\"] = end\n",
    "            velocity_widget.set_config(config)\n",
    "            velocity_widget.plot_time_series()\n",
    "            \n",
    "def update_plottype(change):\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            config[\"color_by\"] = plot_type.value\n",
    "            velocity_widget.set_config(config)\n",
    "            velocity_widget.plot_time_series()\n",
    "            \n",
    "def update_mean(change):\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            config[\"running_mean\"] = include_running_mean.value\n",
    "            velocity_widget.set_config(config)\n",
    "            velocity_widget.plot_time_series()\n",
    "            \n",
    "def add_point(event):\n",
    "    coords = (latitude.value, longitude.value)\n",
    "    velocity_widget.add_point(coords)\n",
    "    \n",
    "def export_ts(event):\n",
    "    velocity_widget.export_data()\n",
    "\n",
    "### --- Call the functions on button clicks --- ###\n",
    "get_points.on_click(velocity_widget.plot_time_series)\n",
    "plot_button.on_click(plotter)\n",
    "clear_button.on_click(velocity_widget.clear_points)\n",
    "export_button.on_click(downloader)\n",
    "add_button.on_click(add_point)\n",
    "dates_range.observe(update_range, 'value')\n",
    "plot_type.observe(update_plottype, 'value')\n",
    "variables.observe(update_variable, 'value')\n",
    "include_running_mean.observe(update_mean, 'value')\n",
    "\n",
    "### --- Plot the widget --- ###\n",
    "layout = widgets.Layout(align_items='stretch',\n",
    "                        display='flex',\n",
    "                        flex_flow='row wrap',\n",
    "                        border='none',\n",
    "                        grid_template_columns=\"repeat(auto-fit, minmax(720px, 1fr))\",\n",
    "                        # grid_template_columns='48% 48%',\n",
    "                        width='99%',\n",
    "                        height='100%')\n",
    "\n",
    "velocity_widget.set_config(config)\n",
    "\n",
    "velocity_widget.fig.canvas.capture_scroll = True\n",
    "\n",
    "# We render the widget\n",
    "widgets.GridBox([\n",
    "                widgets.VBox([velocity_widget.map,\n",
    "                            widgets.HBox([latitude, longitude, add_button, clear_button], layout=widgets.Layout(align_items=\"flex-start\",\n",
    "                                                                                                                flex_flow='row wrap'))],\n",
    "                            \n",
    "                            layout=widgets.Layout(min_width=\"100%\",\n",
    "                                                    display=\"flex\",\n",
    "                                                    # height=\"100%\",\n",
    "                                                    # max_height=\"100%\",\n",
    "                                                    max_width=\"100%\")),\n",
    "                widgets.VBox([\n",
    "                            widgets.HBox([get_points, export_button, plot_button, data_link])\n",
    "                            ], layout=widgets.Layout(min_width=\"720px\",\n",
    "                                                        overflow='scroll',\n",
    "                                                        max_width=\"100%\",\n",
    "                                                        display='flex'))],\n",
    "                layout=layout)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "2b7814ffd776579197ebe28bd31711c0ecb3137f77c21042d9f4200b395566ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
